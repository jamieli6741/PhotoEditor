#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯ä»¶ç…§è‡ªåŠ¨ç”Ÿæˆå™¨ - ç®€åŒ–ç‰ˆ
æ”¯æŒäººè„¸æ£€æµ‹ã€èƒŒæ™¯æ›¿æ¢ã€å°ºå¯¸è°ƒæ•´ã€æ‰¹é‡å¤„ç†ç­‰åŠŸèƒ½
"""

import cv2
import numpy as np
from PIL import Image, ImageFilter, ImageEnhance
import argparse
import os
import json
from typing import Tuple, Optional, List, Dict
import logging
from pathlib import Path
import concurrent.futures
from functools import lru_cache

# å°è¯•å¯¼å…¥MediaPipeå’Œå…¶ä»–å¯é€‰ä¾èµ–
try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False

try:
    from rembg import remove
    REMBG_AVAILABLE = True
except ImportError:
    REMBG_AVAILABLE = False

# é¢„è®¾è¯ä»¶ç…§è§„æ ¼
PHOTO_SPECS = {
    "1å¯¸": (295, 413),
    "2å¯¸": (413, 579),
    "å°1å¯¸": (260, 378),
    "å°2å¯¸": (378, 522),
    "æŠ¤ç…§": (390, 567),
    "ç­¾è¯": (390, 567),
    "é©¾ç…§": (260, 378),
    "èº«ä»½è¯": (358, 441),
    "ç¤¾ä¿å¡": (358, 441),
    "å·¥ä½œè¯": (413, 579),
    "ç¾å›½ç­¾è¯": (600, 600)  # 2Ã—2è‹±å¯¸ï¼Œ51Ã—51mmï¼Œæ­£æ–¹å½¢
}

# å¸¸ç”¨èƒŒæ™¯é¢œè‰²
BG_COLORS = {
    "white": (255, 255, 255),
    "blue": (255, 0, 0),
    "red": (0, 0, 255),
    "light_blue": (255, 191, 0),
    "gray": (128, 128, 128)
}

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class IDPhotoGenerator:
    def __init__(self, config_path: Optional[str] = None):
        """åˆå§‹åŒ–è¯ä»¶ç…§ç”Ÿæˆå™¨"""
        self.config = self._load_config(config_path)
        self._init_detectors()
        
    def _load_config(self, config_path: Optional[str]) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            "face_ratio": 0.75,
            "face_position_y": 0.35,
            "enhance_params": {
                "sharpen_radius": 1,
                "sharpen_percent": 150,
                "contrast_factor": 1.1,
                "brightness_offset": 10
            },
            "background_removal": {
                "method": "grabcut",
                "grabcut_iterations": 5,
                "morphology_kernel_size": 3
            },
            "face_detection": {
                "mediapipe_confidence": 0.5,
                "opencv_scale_factor": 1.1,
                "opencv_min_neighbors": 5
            },
            "us_visa": {
                "face_ratio": 0.60,  # ç¾å›½ç­¾è¯è¦æ±‚50%-69%ï¼Œè®¾ç½®ä¸º60%
                "face_position_y": 0.30
            }
        }
        
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                default_config.update(user_config)
        
        return default_config

    def _init_detectors(self):
        """åˆå§‹åŒ–æ£€æµ‹å™¨"""
        # OpenCVäººè„¸æ£€æµ‹å™¨
        self.opencv_detector = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        
        # MediaPipeæ£€æµ‹å™¨
        if MEDIAPIPE_AVAILABLE:
            self.mp_face_detection = mp.solutions.face_detection
            self.mp_face_mesh = mp.solutions.face_mesh
            self.use_mediapipe = True
            logger.info("MediaPipeäººè„¸æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
        else:
            self.use_mediapipe = False
            logger.info("MediaPipeä¸å¯ç”¨ï¼Œä½¿ç”¨OpenCVè¿›è¡Œäººè„¸æ£€æµ‹")

    @lru_cache(maxsize=10)
    def _get_face_detector_config(self) -> Dict:
        """è·å–äººè„¸æ£€æµ‹é…ç½®ï¼ˆç¼“å­˜ï¼‰"""
        return self.config.get("face_detection", {})

    def detect_face_with_landmarks(self, image: np.ndarray) -> Tuple[Optional[Tuple[int, int, int, int]], Optional[np.ndarray]]:
        """
        æ£€æµ‹äººè„¸å¹¶è¿”å›å…³é”®ç‚¹ä¿¡æ¯
        
        Returns:
            (face_bbox, landmarks) - äººè„¸è¾¹ç•Œæ¡†å’Œå…³é”®ç‚¹
        """
        if self.use_mediapipe:
            return self._detect_face_mediapipe_advanced(image)
        else:
            face_bbox = self.detect_face_opencv(image)
            return face_bbox, None

    def _detect_face_mediapipe_advanced(self, image: np.ndarray) -> Tuple[Optional[Tuple[int, int, int, int]], Optional[np.ndarray]]:
        """å¢å¼ºç‰ˆMediaPipeäººè„¸æ£€æµ‹"""
        config = self._get_face_detector_config()
        confidence = config.get("mediapipe_confidence", 0.5)
        
        # äººè„¸æ£€æµ‹
        with self.mp_face_detection.FaceDetection(
            model_selection=1, 
            min_detection_confidence=confidence
        ) as face_detection:
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            results = face_detection.process(rgb_image)
            
            face_bbox = None
            if results.detections:
                detection = results.detections[0]
                bbox = detection.location_data.relative_bounding_box
                h, w, _ = image.shape
                x = int(bbox.xmin * w)
                y = int(bbox.ymin * h)
                width = int(bbox.width * w)
                height = int(bbox.height * h)
                face_bbox = (x, y, width, height)
        
        # è·å–é¢éƒ¨å…³é”®ç‚¹
        landmarks = None
        if face_bbox:
            with self.mp_face_mesh.FaceMesh(
                static_image_mode=True,
                max_num_faces=1,
                refine_landmarks=True,
                min_detection_confidence=confidence
            ) as face_mesh:
                results = face_mesh.process(rgb_image)
                if results.multi_face_landmarks:
                    landmarks_3d = results.multi_face_landmarks[0]
                    h, w, _ = image.shape
                    landmarks = []
                    for landmark in landmarks_3d.landmark:
                        x = int(landmark.x * w)
                        y = int(landmark.y * h)
                        landmarks.append([x, y])
                    landmarks = np.array(landmarks)
        
        return face_bbox, landmarks

    def detect_face_opencv(self, image: np.ndarray) -> Optional[Tuple[int, int, int, int]]:
        """OpenCVäººè„¸æ£€æµ‹ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # ç›´æ–¹å›¾å‡è¡¡åŒ–æé«˜æ£€æµ‹æ•ˆæœ
        gray = cv2.equalizeHist(gray)
        
        config = self._get_face_detector_config()
        scale_factor = config.get("opencv_scale_factor", 1.1)
        min_neighbors = config.get("opencv_min_neighbors", 5)
        
        # å¤šå°ºåº¦æ£€æµ‹
        faces = self.opencv_detector.detectMultiScale(
            gray,
            scaleFactor=scale_factor,
            minNeighbors=min_neighbors,
            minSize=(30, 30),
            flags=cv2.CASCADE_SCALE_IMAGE
        )
        
        if len(faces) > 0:
            # é€‰æ‹©æœ€å¤§ä¸”æœ€å±…ä¸­çš„äººè„¸
            h, w = gray.shape
            center_x, center_y = w // 2, h // 2
            
            best_face = None
            best_score = -1
            
            for face in faces:
                x, y, fw, fh = face
                face_center_x = x + fw // 2
                face_center_y = y + fh // 2
                
                # è®¡ç®—ç»¼åˆå¾—åˆ†ï¼šå°ºå¯¸ + ä½ç½®
                size_score = (fw * fh) / (w * h)  # ç›¸å¯¹å¤§å°
                distance = np.sqrt((face_center_x - center_x)**2 + (face_center_y - center_y)**2)
                position_score = 1 / (1 + distance / min(w, h))  # è·ç¦»ä¸­å¿ƒçš„è¿œè¿‘
                
                total_score = size_score * 0.7 + position_score * 0.3
                
                if total_score > best_score:
                    best_score = total_score
                    best_face = face
            
            return tuple(best_face)
        
        return None

    def remove_background_advanced(self, image: np.ndarray, method: str = "auto") -> np.ndarray:
        """
        å¢å¼ºç‰ˆèƒŒæ™¯ç§»é™¤
        
        Args:
            image: è¾“å…¥å›¾åƒ
            method: ç§»é™¤æ–¹æ³• ("auto", "rembg", "grabcut", "threshold")
        """
        if method == "auto":
            # è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ–¹æ³•
            if REMBG_AVAILABLE:
                method = "rembg"
            else:
                method = "grabcut"
        
        if method == "rembg" and REMBG_AVAILABLE:
            return self._remove_background_rembg(image)
        elif method == "grabcut":
            return self._remove_background_grabcut_enhanced(image)
        else:
            return self._remove_background_threshold(image)

    def _remove_background_rembg(self, image: np.ndarray) -> np.ndarray:
        """ä½¿ç”¨rembgåº“ç§»é™¤èƒŒæ™¯ï¼ˆAIæ–¹æ³•ï¼Œæ•ˆæœæœ€å¥½ï¼‰"""
        # è½¬æ¢ä¸ºPILå›¾åƒ
        pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        
        # ä½¿ç”¨rembgç§»é™¤èƒŒæ™¯
        result_pil = remove(pil_image)
        
        # è½¬æ¢å›OpenCVæ ¼å¼
        result_array = np.array(result_pil)
        
        if result_array.shape[2] == 4:  # å·²æœ‰alphaé€šé“
            return cv2.cvtColor(result_array, cv2.COLOR_RGBA2BGRA)
        else:
            # æ·»åŠ alphaé€šé“
            height, width = result_array.shape[:2]
            result_bgra = np.zeros((height, width, 4), dtype=np.uint8)
            result_bgra[:, :, :3] = cv2.cvtColor(result_array, cv2.COLOR_RGB2BGR)
            result_bgra[:, :, 3] = 255
            return result_bgra

    def _remove_background_grabcut_enhanced(self, image: np.ndarray) -> np.ndarray:
        """å¢å¼ºç‰ˆGrabCutèƒŒæ™¯ç§»é™¤"""
        height, width = image.shape[:2]
        mask = np.zeros((height, width), np.uint8)
        
        # æ”¹è¿›çš„å‰æ™¯åŒºåŸŸä¼°è®¡
        padding_x = max(width // 8, 20)
        padding_y = max(height // 10, 20)
        rect = (padding_x, padding_y, width - 2*padding_x, height - 2*padding_y)
        
        bgd_model = np.zeros((1, 65), np.float64)
        fgd_model = np.zeros((1, 65), np.float64)
        
        config = self.config.get("background_removal", {})
        iterations = config.get("grabcut_iterations", 5)
        
        # åº”ç”¨GrabCut
        cv2.grabCut(image, mask, rect, bgd_model, fgd_model, iterations, cv2.GC_INIT_WITH_RECT)
        
        # åˆ›å»ºäºŒå€¼æ©ç 
        mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')
        
        # å½¢æ€å­¦æ“ä½œä¼˜åŒ–æ©ç 
        kernel_size = config.get("morphology_kernel_size", 3)
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))
        mask2 = cv2.morphologyEx(mask2, cv2.MORPH_CLOSE, kernel)
        mask2 = cv2.morphologyEx(mask2, cv2.MORPH_OPEN, kernel)
        
        # é«˜æ–¯æ¨¡ç³Šå¹³æ»‘è¾¹ç¼˜
        mask2 = cv2.GaussianBlur(mask2.astype(np.float32), (3, 3), 1)
        mask2 = (mask2 * 255).astype(np.uint8)
        
        # åˆ›å»ºç»“æœå›¾åƒ
        result = np.zeros((height, width, 4), dtype=np.uint8)
        result[:, :, :3] = image
        result[:, :, 3] = mask2
        
        return result

    def _remove_background_threshold(self, image: np.ndarray) -> np.ndarray:
        """é˜ˆå€¼æ–¹æ³•èƒŒæ™¯ç§»é™¤ï¼ˆç®€å•æ–¹æ³•ï¼‰"""
        # è½¬æ¢ä¸ºHSVè¿›è¡Œæ›´å¥½çš„é¢œè‰²åˆ†å‰²
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        
        # åˆ›å»ºæ©ç ï¼ˆè¿™é‡Œå‡è®¾èƒŒæ™¯æ˜¯ç›¸å¯¹å‡åŒ€çš„ï¼‰
        # å¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚è°ƒæ•´é˜ˆå€¼
        lower = np.array([0, 0, 200])  # å‡è®¾èƒŒæ™¯æ˜¯äº®è‰²
        upper = np.array([180, 30, 255])
        mask = cv2.inRange(hsv, lower, upper)
        
        # åè½¬æ©ç ï¼ˆå‰æ™¯ä¸ºç™½ï¼ŒèƒŒæ™¯ä¸ºé»‘ï¼‰
        mask = cv2.bitwise_not(mask)
        
        # å½¢æ€å­¦æ“ä½œæ¸…ç†æ©ç 
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
        
        # åˆ›å»ºå¸¦alphaé€šé“çš„ç»“æœ
        result = np.zeros((image.shape[0], image.shape[1], 4), dtype=np.uint8)
        result[:, :, :3] = image
        result[:, :, 3] = mask
        
        return result

    def add_background(self, image_with_alpha: np.ndarray, bg_color: Tuple[int, int, int]) -> np.ndarray:
        """
        ä¸ºå¸¦æœ‰alphaé€šé“çš„å›¾åƒæ·»åŠ èƒŒæ™¯è‰²
        
        Args:
            image_with_alpha: å¸¦æœ‰alphaé€šé“çš„å›¾åƒ (BGRAæ ¼å¼)
            bg_color: èƒŒæ™¯é¢œè‰² (B, G, R)
        
        Returns:
            æ·»åŠ èƒŒæ™¯åçš„å›¾åƒ (BGRæ ¼å¼)
        """
        if image_with_alpha.shape[2] != 4:
            # å¦‚æœæ²¡æœ‰alphaé€šé“ï¼Œç›´æ¥è¿”å›åŸå›¾
            return image_with_alpha
        
        height, width = image_with_alpha.shape[:2]
        
        # åˆ†ç¦»RGBå’ŒAlphaé€šé“
        bgr_image = image_with_alpha[:, :, :3]
        alpha_channel = image_with_alpha[:, :, 3]
        
        # åˆ›å»ºèƒŒæ™¯å›¾åƒ
        background = np.full((height, width, 3), bg_color, dtype=np.uint8)
        
        # å°†alphaé€šé“å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        alpha_normalized = alpha_channel.astype(np.float32) / 255.0
        
        # æ‰©å±•alphaé€šé“ç»´åº¦ä»¥ä¾¿å¹¿æ’­
        alpha_3d = np.stack([alpha_normalized] * 3, axis=2)
        
        # æ‰§è¡Œalphaæ··åˆ
        # result = foreground * alpha + background * (1 - alpha)
        result = (bgr_image.astype(np.float32) * alpha_3d + 
                  background.astype(np.float32) * (1 - alpha_3d))
        
        return result.astype(np.uint8)

    def enhance_image_advanced(self, image: np.ndarray) -> np.ndarray:
        """
        é«˜çº§å›¾åƒå¢å¼º
        """
        # è½¬æ¢ä¸ºPILè¿›è¡Œå¤„ç†
        pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        
        config = self.config.get("enhance_params", {})
        
        # é”åŒ–
        radius = config.get("sharpen_radius", 1)
        percent = config.get("sharpen_percent", 150)
        pil_image = pil_image.filter(ImageFilter.UnsharpMask(radius=radius, percent=percent, threshold=3))
        
        # å¯¹æ¯”åº¦å¢å¼º
        contrast_factor = config.get("contrast_factor", 1.1)
        enhancer = ImageEnhance.Contrast(pil_image)
        pil_image = enhancer.enhance(contrast_factor)
        
        # äº®åº¦è°ƒæ•´
        brightness_factor = 1.0 + config.get("brightness_offset", 10) / 255.0
        enhancer = ImageEnhance.Brightness(pil_image)
        pil_image = enhancer.enhance(brightness_factor)
        
        # è‰²å½©é¥±å’Œåº¦å¾®è°ƒ
        enhancer = ImageEnhance.Color(pil_image)
        pil_image = enhancer.enhance(1.05)
        
        # è½¬æ¢å›OpenCVæ ¼å¼
        enhanced = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
        
        # é™å™ªå¤„ç†
        enhanced = cv2.fastNlMeansDenoisingColored(enhanced, None, 3, 3, 7, 21)
        
        return enhanced

    def adjust_face_position_advanced(self, image: np.ndarray, face_bbox: Tuple[int, int, int, int],
                                    landmarks: Optional[np.ndarray] = None,
                                    target_ratio: float = 0.75, 
                                    target_size: Tuple[int, int] = (295, 413)) -> np.ndarray:
        """
        é«˜çº§äººè„¸ä½ç½®è°ƒæ•´ï¼ˆè€ƒè™‘å…³é”®ç‚¹ä¿¡æ¯ï¼‰
        """
        x, y, w, h = face_bbox
        
        # å¦‚æœæœ‰å…³é”®ç‚¹ï¼Œä½¿ç”¨çœ¼éƒ¨ä½ç½®è¿›è¡Œæ›´ç²¾ç¡®çš„å®šä½
        if landmarks is not None and len(landmarks) > 468:  # MediaPipeé¢éƒ¨ç½‘æ ¼æœ‰468ä¸ªç‚¹
            # è·å–çœ¼éƒ¨å…³é”®ç‚¹ï¼ˆç®€åŒ–ç‰ˆï¼‰
            left_eye = landmarks[33:43].mean(axis=0)  # å·¦çœ¼åŒºåŸŸ
            right_eye = landmarks[362:372].mean(axis=0)  # å³çœ¼åŒºåŸŸ
            eye_center = ((left_eye + right_eye) / 2).astype(int)
            
            # ä½¿ç”¨çœ¼éƒ¨ä¸­å¿ƒä½œä¸ºå‚è€ƒç‚¹
            face_center_x, face_center_y = eye_center
            
            # æ ¹æ®çœ¼éƒ¨ä½ç½®è°ƒæ•´äººè„¸æ¡†
            face_height = max(h, int(abs(landmarks[10][1] - landmarks[152][1]) * 1.3))  # ä¸‹å·´åˆ°é¢å¤´
        else:
            # ä½¿ç”¨åŸæœ‰çš„äººè„¸ä¸­å¿ƒ
            face_center_x = x + w // 2
            face_center_y = y + h // 2
            face_height = h
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        target_face_height = int(target_size[1] * target_ratio)
        scale_factor = target_face_height / face_height
        
        # ç¼©æ”¾å›¾åƒ
        new_height, new_width = image.shape[:2]
        new_height = int(new_height * scale_factor)
        new_width = int(new_width * scale_factor)
        
        scaled_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LANCZOS4)
        
        # é‡æ–°è®¡ç®—äººè„¸ä¸­å¿ƒ
        new_face_center_x = int(face_center_x * scale_factor)
        new_face_center_y = int(face_center_y * scale_factor)
        
        # åˆ›å»ºç”»å¸ƒ
        canvas = np.full((target_size[1], target_size[0], 3), 255, dtype=np.uint8)
        
        # è®¡ç®—ç²˜è´´ä½ç½®
        face_position_y = self.config.get("face_position_y", 0.35)
        paste_x = target_size[0] // 2 - new_face_center_x
        paste_y = int(target_size[1] * face_position_y) - new_face_center_y
        
        # æ‰§è¡Œç²˜è´´
        self._paste_image_to_canvas(scaled_image, canvas, paste_x, paste_y)
        
        return canvas

    def _paste_image_to_canvas(self, src_image: np.ndarray, canvas: np.ndarray, 
                             paste_x: int, paste_y: int):
        """å°†æºå›¾åƒç²˜è´´åˆ°ç”»å¸ƒä¸Š"""
        src_h, src_w = src_image.shape[:2]
        canvas_h, canvas_w = canvas.shape[:2]
        
        # è®¡ç®—å®é™…ç²˜è´´åŒºåŸŸ
        src_x1 = max(0, -paste_x)
        src_y1 = max(0, -paste_y)
        src_x2 = min(src_w, src_x1 + canvas_w - max(0, paste_x))
        src_y2 = min(src_h, src_y1 + canvas_h - max(0, paste_y))
        
        dst_x1 = max(0, paste_x)
        dst_y1 = max(0, paste_y)
        dst_x2 = dst_x1 + (src_x2 - src_x1)
        dst_y2 = dst_y1 + (src_y2 - src_y1)
        
        if src_x2 > src_x1 and src_y2 > src_y1:
            canvas[dst_y1:dst_y2, dst_x1:dst_x2] = src_image[src_y1:src_y2, src_x1:src_x2]

    def generate_id_photo_advanced(self, input_path: str, output_path: str,
                                 spec_name: str = "1å¯¸", 
                                 bg_color_name: str = "white",
                                 face_ratio: Optional[float] = None,
                                 remove_bg_method: str = "auto") -> Dict:
        """
        é«˜çº§è¯ä»¶ç…§ç”Ÿæˆ
        
        Returns:
            å¤„ç†ç»“æœä¿¡æ¯å­—å…¸
        """
        result = {
            "success": False,
            "message": "",
            "processing_time": 0,
            "face_detected": False,
            "output_size": None
        }
        
        import time
        start_time = time.time()
        
        try:
            # è¯»å–å›¾åƒ
            image = cv2.imread(input_path)
            if image is None:
                result["message"] = f"æ— æ³•è¯»å–å›¾åƒ: {input_path}"
                return result
            
            logger.info(f"å¼€å§‹å¤„ç†å›¾åƒ: {input_path}")
            
            # è·å–è§„æ ¼å’Œé¢œè‰²
            target_size = PHOTO_SPECS.get(spec_name, PHOTO_SPECS["1å¯¸"])
            bg_color = BG_COLORS.get(bg_color_name, BG_COLORS["white"])
            
            # å¦‚æœæ˜¯ç¾å›½ç­¾è¯ä¸”æ²¡æœ‰æŒ‡å®šface_ratioï¼Œä½¿ç”¨ç‰¹æ®Šé…ç½®
            if spec_name == "ç¾å›½ç­¾è¯" and face_ratio is None:
                face_ratio = self.config.get("us_visa", {}).get("face_ratio", 0.60)
                # åŒæ—¶æ›´æ–°face_position_y
                self.config["face_position_y"] = self.config.get("us_visa", {}).get("face_position_y", 0.30)
            elif face_ratio is None:
                face_ratio = self.config["face_ratio"]
            
            # æ£€æµ‹äººè„¸å’Œå…³é”®ç‚¹
            face_bbox, landmarks = self.detect_face_with_landmarks(image)
            if face_bbox is None:
                result["message"] = "æœªæ£€æµ‹åˆ°äººè„¸"
                return result
            
            result["face_detected"] = True
            logger.info(f"æ£€æµ‹åˆ°äººè„¸: {face_bbox}")
            
            processed_image = image.copy()
            
            # èƒŒæ™¯ç§»é™¤å’Œæ›¿æ¢
            if remove_bg_method != "none":
                logger.info("å¤„ç†èƒŒæ™¯...")
                image_with_alpha = self.remove_background_advanced(processed_image, remove_bg_method)
                processed_image = self.add_background(image_with_alpha, bg_color)
            
            # è°ƒæ•´äººè„¸ä½ç½®
            logger.info("è°ƒæ•´äººè„¸ä½ç½®å’Œæ¯”ä¾‹...")
            final_image = self.adjust_face_position_advanced(
                processed_image, face_bbox, landmarks, face_ratio, target_size
            )
            
            # å›¾åƒå¢å¼º
            logger.info("å¢å¼ºå›¾åƒè´¨é‡...")
            final_image = self.enhance_image_advanced(final_image)
            
            # ä¿å­˜ç»“æœ
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            success = cv2.imwrite(output_path, final_image, [cv2.IMWRITE_JPEG_QUALITY, 95])
            
            if success:
                result["success"] = True
                result["message"] = "è¯ä»¶ç…§ç”ŸæˆæˆåŠŸ"
                result["output_size"] = target_size
                logger.info(f"è¯ä»¶ç…§å·²ä¿å­˜åˆ°: {output_path}")
            else:
                result["message"] = "ä¿å­˜å›¾åƒå¤±è´¥"
            
        except Exception as e:
            result["message"] = f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"
            logger.error(result["message"])
        
        finally:
            result["processing_time"] = time.time() - start_time
        
        return result

    def batch_process(self, input_dir: str, output_dir: str, **kwargs) -> List[Dict]:
        """æ‰¹é‡å¤„ç†è¯ä»¶ç…§"""
        input_path = Path(input_dir)
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # æ”¯æŒçš„å›¾åƒæ ¼å¼
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
        image_files = [f for f in input_path.iterdir() 
                      if f.suffix.lower() in image_extensions]
        
        results = []
        
        # å¤šçº¿ç¨‹å¤„ç†
        max_workers = min(4, len(image_files))  # é™åˆ¶çº¿ç¨‹æ•°
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_file = {}
            
            for image_file in image_files:
                output_file = output_path / f"{image_file.stem}_id{image_file.suffix}"
                future = executor.submit(
                    self.generate_id_photo_advanced,
                    str(image_file),
                    str(output_file),
                    **kwargs
                )
                future_to_file[future] = image_file
            
            for future in concurrent.futures.as_completed(future_to_file):
                image_file = future_to_file[future]
                try:
                    result = future.result()
                    result["input_file"] = str(image_file)
                    results.append(result)
                    
                    status = "âœ…" if result["success"] else "âŒ"
                    print(f"{status} {image_file.name}: {result['message']}")
                    
                except Exception as e:
                    results.append({
                        "input_file": str(image_file),
                        "success": False,
                        "message": f"å¤„ç†å¼‚å¸¸: {str(e)}"
                    })
                    print(f"âŒ {image_file.name}: å¤„ç†å¼‚å¸¸")
        
        return results


def create_default_config(config_path: str):
    """åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶"""
    default_config = {
        "face_ratio": 0.75,
        "face_position_y": 0.35,
        "us_visa": {
            "face_ratio": 0.60,
            "face_position_y": 0.30
        },
        "enhance_params": {
            "sharpen_radius": 1,
            "sharpen_percent": 150,
            "contrast_factor": 1.1,
            "brightness_offset": 10
        },
        "background_removal": {
            "method": "grabcut",
            "grabcut_iterations": 5,
            "morphology_kernel_size": 3
        },
        "face_detection": {
            "mediapipe_confidence": 0.5,
            "opencv_scale_factor": 1.1,
            "opencv_min_neighbors": 5
        }
    }
    
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(default_config, f, ensure_ascii=False, indent=2)
    
    print(f"é»˜è®¤é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è¯ä»¶ç…§è‡ªåŠ¨ç”Ÿæˆå™¨')
    parser.add_argument('input', help='è¾“å…¥å›¾ç‰‡è·¯å¾„æˆ–ç›®å½•')
    parser.add_argument('output', help='è¾“å‡ºå›¾ç‰‡è·¯å¾„æˆ–ç›®å½•')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--spec', choices=list(PHOTO_SPECS.keys()), default='1å¯¸',
                        help='è¯ä»¶ç…§è§„æ ¼')
    parser.add_argument('--bg-color', choices=list(BG_COLORS.keys()), default='white',
                        help='èƒŒæ™¯é¢œè‰²')
    parser.add_argument('--face-ratio', type=float, help='äººè„¸å ç”»é¢é«˜åº¦çš„æ¯”ä¾‹')
    parser.add_argument('--bg-method', choices=['auto', 'rembg', 'grabcut', 'threshold', 'none'],
                        default='auto', help='èƒŒæ™¯ç§»é™¤æ–¹æ³•')
    
    # é«˜çº§é€‰é¡¹
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--batch', action='store_true', help='æ‰¹é‡å¤„ç†æ¨¡å¼')
    parser.add_argument('--create-config', help='åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶
    if args.create_config:
        create_default_config(args.create_config)
        return
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = IDPhotoGenerator(args.config)
    
    if args.batch:
        # æ‰¹é‡å¤„ç†
        if not os.path.isdir(args.input):
            print("âŒ æ‰¹é‡æ¨¡å¼éœ€è¦è¾“å…¥ç›®å½•")
            return
        
        results = generator.batch_process(
            args.input, args.output,
            spec_name=args.spec,
            bg_color_name=args.bg_color,
            face_ratio=args.face_ratio,
            remove_bg_method=args.bg_method
        )
        
        # è¾“å‡ºæ‰¹é‡å¤„ç†ç»Ÿè®¡
        success_count = sum(1 for r in results if r["success"])
        total_count = len(results)
        print(f"\nğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ: {success_count}/{total_count} æˆåŠŸ")
        
        # ä¿å­˜å¤„ç†æŠ¥å‘Š
        report_path = os.path.join(args.output, "processing_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“‹ å¤„ç†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
    else:
        # å•å¼ å¤„ç†
        if not os.path.exists(args.input):
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
            return
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(args.output)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # å¤„ç†å•å¼ å›¾ç‰‡
        result = generator.generate_id_photo_advanced(
            input_path=args.input,
            output_path=args.output,
            spec_name=args.spec,
            bg_color_name=args.bg_color,
            face_ratio=args.face_ratio,
            remove_bg_method=args.bg_method
        )
        
        # è¾“å‡ºç»“æœ
        status = "âœ…" if result["success"] else "âŒ"
        print(f"{status} {result['message']}")
        
        if result["success"]:
            print(f"ğŸ“ è¾“å‡ºå°ºå¯¸: {result['output_size'][0]}Ã—{result['output_size'][1]}")
            print(f"â±ï¸  å¤„ç†æ—¶é—´: {result['processing_time']:.2f}ç§’")
            
            # ç¾å›½ç­¾è¯ç‰¹æ®Šæç¤º
            if args.spec == "ç¾å›½ç­¾è¯":
                print("ğŸ‡ºğŸ‡¸ ç¾å›½ç­¾è¯ç…§ç‰‡è¦æ±‚:")
                print("   - å¤´éƒ¨åº”å ç…§ç‰‡é«˜åº¦çš„50%-69%")
                print("   - çœ¼éƒ¨ä½ç½®åœ¨ç…§ç‰‡ä¸­å¿ƒçº¿ä¸Šæ–¹")
                print("   - ä¿æŒä¸­æ€§è¡¨æƒ…ï¼Œå˜´å·´é—­åˆ")
                print("   - ç›´è§†ç›¸æœºï¼Œçœ¼ç›æ¸…æ™°å¯è§")
        
        # æ˜¾ç¤ºå¯ç”¨é€‰é¡¹æç¤º
        if not result["success"] and not result["face_detected"]:
            print("\nğŸ’¡ æç¤º:")
            print("1. ç¡®ä¿ç…§ç‰‡ä¸­æœ‰æ¸…æ™°å¯è§çš„äººè„¸")
            print("2. äººè„¸åº”è¯¥æ­£é¢æœå‘ç›¸æœº")
            print("3. å…‰çº¿å……è¶³ï¼Œé¿å…é˜´å½±é®æŒ¡")
            print("4. å¯ä»¥å°è¯•ä¸åŒçš„æ£€æµ‹æ–¹æ³•ï¼ˆå®‰è£… mediapipe: pip install mediapipeï¼‰")


def print_usage_examples():
    """æ‰“å°ä½¿ç”¨ç¤ºä¾‹"""
    examples = """
ğŸš€ è¯ä»¶ç…§ç”Ÿæˆå™¨ä½¿ç”¨ç¤ºä¾‹:

åŸºæœ¬ä½¿ç”¨:
  python id_photo_generator.py input.jpg output.jpg

æŒ‡å®šè¯ä»¶ç…§è§„æ ¼:
  python id_photo_generator.py input.jpg output.jpg --spec 2å¯¸
  python id_photo_generator.py input.jpg output.jpg --spec ç¾å›½ç­¾è¯

æŒ‡å®šèƒŒæ™¯é¢œè‰²:
  python id_photo_generator.py input.jpg output.jpg --bg-color blue

ç¾å›½ç­¾è¯ç…§ç‰‡ç”Ÿæˆ:
  python id_photo_generator.py input.jpg us_visa.jpg --spec ç¾å›½ç­¾è¯
  python id_photo_generator.py input.jpg us_visa.jpg --spec ç¾å›½ç­¾è¯ --face-ratio 0.60

ä½¿ç”¨AIèƒŒæ™¯ç§»é™¤ (éœ€è¦å®‰è£… rembg):
  python id_photo_generator.py input.jpg output.jpg --bg-method rembg

æ‰¹é‡å¤„ç†:
  python id_photo_generator.py input_dir/ output_dir/ --batch --spec ç¾å›½ç­¾è¯

åˆ›å»ºé…ç½®æ–‡ä»¶:
  python id_photo_generator.py --create-config config.json

ä½¿ç”¨è‡ªå®šä¹‰é…ç½®:
  python id_photo_generator.py input.jpg output.jpg --config config.json

ğŸ“ æ”¯æŒçš„è¯ä»¶ç…§è§„æ ¼:
  1å¯¸ (295Ã—413), 2å¯¸ (413Ã—579), å°1å¯¸ (260Ã—378)
  å°2å¯¸ (378Ã—522), æŠ¤ç…§ (390Ã—567), ç­¾è¯ (390Ã—567)
  é©¾ç…§ (260Ã—378), èº«ä»½è¯ (358Ã—441), ç¤¾ä¿å¡ (358Ã—441)
  ç¾å›½ç­¾è¯ (600Ã—600) - 2Ã—2è‹±å¯¸æ­£æ–¹å½¢æ ¼å¼

ğŸ¨ æ”¯æŒçš„èƒŒæ™¯é¢œè‰²:
  white (ç™½è‰²), blue (è“è‰²), red (çº¢è‰²)
  light_blue (æµ…è“è‰²), gray (ç°è‰²)

âš™ï¸ èƒŒæ™¯ç§»é™¤æ–¹æ³•:
  auto (è‡ªåŠ¨é€‰æ‹©), rembg (AIæ–¹æ³•ï¼Œæ•ˆæœæœ€å¥½)
  grabcut (ä¼ ç»Ÿæ–¹æ³•), threshold (é˜ˆå€¼æ–¹æ³•)
  none (ä¿ç•™åŸèƒŒæ™¯)

ğŸ‡ºğŸ‡¸ ç¾å›½ç­¾è¯ç…§ç‰‡ç‰¹æ®Šè¦æ±‚:
  - å°ºå¯¸: 2Ã—2è‹±å¯¸ (600Ã—600åƒç´ )
  - å¤´éƒ¨æ¯”ä¾‹: 50%-69% (é»˜è®¤60%)
  - èƒŒæ™¯: ç™½è‰²
  - è¡¨æƒ…: è‡ªç„¶ï¼Œå˜´å·´é—­åˆ
  - çœ¼ç›: ç›´è§†ç›¸æœºï¼Œæ¸…æ™°å¯è§
  - å¤´é¥°: é™¤å®—æ•™åŸå› å¤–ä¸å…è®¸
  - çœ¼é•œ: å¯ä»¥ä½©æˆ´ï¼Œä½†ä¸èƒ½åå…‰
"""
    print(examples)


if __name__ == "__main__":
    import sys
    
    # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    if len(sys.argv) == 1:
        print_usage_examples()
    else:
        main()